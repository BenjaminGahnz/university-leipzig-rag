#  University Leipzig RAG Chat System

Ein lokales RAG (Retrieval-Augmented Generation) System fÃ¼r die Studiendokumente der UniversitÃ¤t Leipzig. Das System ermÃ¶glicht es, Fragen zu Studien- und PrÃ¼fungsordnungen zu stellen und erhÃ¤lt Antworten basierend auf den offiziellen Dokumenten.

## Features

- **Lokale AusfÃ¼hrung**: VollstÃ¤ndig lokal mit Ollama LLMs
- **Deutsche SprachunterstÃ¼tzung**: Optimiert fÃ¼r deutsche UniversitÃ¤tsdokumente  
- **Intelligente Dokumentenverarbeitung**: Automatische PDF-Verarbeitung und Chunking
- **Web-basierte Chat-OberflÃ¤che**: Moderne Streamlit-UI
- **Quellenangaben**: Transparente Nachverfolgung der Antwortquellen
- **Automatisches Web Scraping**: Download von UniversitÃ¤tsdokumenten
- **Modulare Architektur**: Erweiterbar und wartbar

##  Voraussetzungen

### Hardware
- **RAM**: Mindestens 8GB (16GB empfohlen fÃ¼r grÃ¶ÃŸere Modelle)
- **Festplatte**: 20GB freier Speicherplatz
- **CPU**: Moderne CPU (Apple Silicon bevorzugt fÃ¼r Performance)

### Software
- **Python**: 3.8 oder hÃ¶her
- **Ollama**: Installiert und lÃ¤uft
- **Git**: FÃ¼r Repository-Kloning

##  Installation

### Schritt 1: Repository klonen
```bash
git clone <repository-url>
cd university-leipzig-rag
```

### Schritt 2: Virtual Environment erstellen
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux  
source venv/bin/activate
```

### Schritt 3: Dependencies installieren
```bash
pip install -r requirements.txt
```

### Schritt 4: spaCy Modell herunterladen
```bash
python -m spacy download de_core_news_lg
```

### Schritt 5: Ollama installieren und einrichten

#### Ollama Installation:
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows: Download von https://ollama.com
```

#### Ollama Modell herunterladen:
```bash
# Ollama starten (in separatem Terminal)
ollama serve

# Modell herunterladen (in neuem Terminal)
ollama pull llama3.1:8b
# oder fÃ¼r weniger RAM:
ollama pull llama3.1:3b
```

### Schritt 6: System einrichten
```bash
python main.py setup
```

## ğŸš€ Verwendung

### 1. Dokumente hinzufÃ¼gen

#### Option A: Manuelle PDF-Dateien
1. PDF-Dateien in `data/pdfs/` ablegen
2. Hierarchische Ordnerstruktur wird unterstÃ¼tzt

#### Option B: Automatisches Web Scraping
```bash
# Mit dem integrierten Scraper
python web_scraper.py --output-dir data/pdfs

# Oder fÃ¼r spezifische URLs
python web_scraper.py --urls https://amb.uni-leipzig.de/startseite-bekanntmachungen.html?kat_id=2001 --output-dir data/pdfs
```

### 2. Dokumente verarbeiten
```bash
python main.py process
```
Dies extrahiert Text aus den PDFs und erstellt Vektorembeddings in ChromaDB.

### 3. System testen
```bash
python main.py test
```
ÃœberprÃ¼ft alle Komponenten und fÃ¼hrt eine Testabfrage durch.

### 4. Chat-Interface starten
```bash
python main.py start
```
Ã–ffnet die Streamlit-OberflÃ¤che unter `http://localhost:8501`

## âš™ï¸ Konfiguration

Die Konfiguration erfolgt Ã¼ber `config.yaml`:

```yaml
# Ollama Einstellungen
ollama:
  base_url: "http://localhost:11434"
  model: "llama2:7b"  # Anpassen je nach verfÃ¼gbarem Modell
  temperature: 0.1

# ChromaDB Einstellungen  
chroma:
  persist_directory: "./data/chroma_db"
  collection_name: "university_regulations"

# Dokumentverarbeitung
documents:
  processing:
    chunk_size: 500
    chunk_overlap: 50
```

## ğŸ“ Projektstruktur

```
university-leipzig-rag/
â”œâ”€â”€ main.py                 # Hauptprogramm mit CLI
â”œâ”€â”€ config.py              # Konfigurationsmanagement
â”œâ”€â”€ config.yaml            # Konfigurationsdatei
â”œâ”€â”€ requirements.txt       # Python-Dependencies
â”œâ”€â”€ streamlit_ui.py        # Chat-BenutzeroberflÃ¤che
â”œâ”€â”€ rag_engine.py          # RAG-Engine mit Ollama
â”œâ”€â”€ document_processor.py  # PDF-Verarbeitung und Vektorisierung
â”œâ”€â”€ web_scraper.py         # Web Scraping fÃ¼r Uni-Dokumente
â”œâ”€â”€ logging_config.py      # Logging-Konfiguration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/             # PDF-Dateien (input)
â”‚   â””â”€â”€ chroma_db/        # Vektor-Datenbank (generiert)
â””â”€â”€ logs/
    â””â”€â”€ rag_system.log    # System-Logs
```

## Fehlerbehebung

### Ollama-Verbindungsprobleme
```bash
# Ollama-Status prÃ¼fen
ollama list

# Ollama neu starten
killall ollama
ollama serve
```

### spaCy-Modell-Probleme
```bash
# Modell erneut herunterladen
python -m spacy download de_core_news_lg --force
```

### ChromaDB-Probleme
```bash
# Datenbank zurÃ¼cksetzen (Vorsicht: lÃ¶scht alle Daten!)
python -c "import shutil; shutil.rmtree('data/chroma_db', ignore_errors=True)"
python main.py process  # Dokumente erneut verarbeiten
```

### Streamlit-Port-Konflikte
```bash
# Anderen Port verwenden
streamlit run streamlit_ui.py --server.port 8502
```

## Verwendungsbeispiele

### Beispielfragen an das System:
- "Wie lange dauert das Masterstudium Ethnologie?"
- "Welche PrÃ¼fungsvorleistungen sind im Bachelorstudium Wirtschaftsinformatik erforderlich?"


### CLI-Kommandos:
```bash
# System-Setup
python main.py setup

# PDFs verarbeiten (mit spezifischem Verzeichnis)
python main.py process --pdf-dir /path/to/pdfs

# System-Status prÃ¼fen
python main.py test

# UI starten
python main.py start

# Web Scraping (eigenstÃ¤ndig)
python web_scraper.py --max-depth 4 --output-dir data/new_pdfs
```

## ğŸ§ª Entwicklung und Testing

### Tests ausfÃ¼hren
```bash
# System-Integrationstests
python main.py test

# Einzelkomponenten testen
python -c "from rag_engine import create_rag_engine; engine = create_rag_engine(); print(engine.check_system_status())"
```

### Logging konfigurieren
```bash
# Debug-Level aktivieren
export LOG_LEVEL=DEBUG
python main.py test
```

### Performance-Optimierung
- **Kleinere Modelle**: Verwenden Sie `llama3.1:3b` fÃ¼r weniger RAM-Verbrauch
- **Chunk-GrÃ¶ÃŸe anpassen**: GrÃ¶ÃŸere Chunks in `config.yaml` fÃ¼r weniger, aber lÃ¤ngere Kontexte
- **GPU-Beschleunigung**: Ollama unterstÃ¼tzt GPU-Beschleunigung automatisch wenn verfÃ¼gbar

## ğŸ› Bekannte Probleme

1. **GroÃŸe PDF-Dateien**: Verarbeitung sehr groÃŸer PDFs kann langsam sein
2. **Ollama-Startup**: Erste Anfrage an Ollama kann lange dauern (Model Loading)
3. **Memory Usage**: GroÃŸe Modelle benÃ¶tigen viel RAM

## ğŸ“„ Lizenz

MIT License -



---

**Entwickelt fÃ¼r Studierende der UniversitÃ¤t Leipzig** ğŸ“